# Homesite Kaggle Competition
This is a brief summary of my approach to solving the Homesite Quote Conversion Kaggle Competition.  The competition details can be found [here] (https://www.kaggle.com/c/homesite-quote-conversion).  My final rank was 298 of 1762.

##Feature Engineering
Feature engineering on this data was a bit challenging given that it was a blind dataset and little or no insight could be intuitively gleaned from the variables.  First, I reformatted the date column into three different categorical variables: month, year and day of the week.  Second, I came across an idea highlighted in the Kaggle Forum (credit [yilisg] (https://www.kaggle.com/c/homesite-quote-conversion/forums/t/18225/two-insights-for-0-96852)) to calculate the number of missing and/or zero-valued elements in each row.  Adding this feature made intuitive sense given that customers who are less inclined to correctly or completely fill out the insurance survey are also probably less inclined to be serious about purchasing an insurance policy.  I added this feature for both each entire row as well as binned subfields within each row (e.g., Property, Personal, etc.).  In total, the train and test datasets had 315 and 314 features, respectively.

##Variable Importance
An extreme gradient boosting (xgb) model was run on the feature engineered dataset in order to reduce the number of features based on their importance.  To achieve this I ran a basic xgb model with eta = 0.02 and nrounds = 2000.  In retrospect I believe the number of rounds was probably too high, but I nevertheless hoped (and assumed) the relative ranking of variable importance would not change too much if I had used fewer trees.  When assessing the variable importance I noticed some features (more than 20!) were not used during the boosting process, which was probably indicative of a high degree of collinearity among the variables.  I drew a somewhat arbitrary line on the gain and said any feature that did not contribute to a gain of at least 5e-5 was deemed not important enough to be included in the final dataset.  This process reduced the number of features by about 21%.  

##Model fitting
Once the feature engineering and variable selection were complete, I started with a 5-fold cross validation on the training dataset using an xgb model.  I held eta and nrounds fixed and ran a custom grid search on subsample, colsample_bytree and max.depth using the caret package in R.  The grid search and xgb parameter hypertuning were inspired by a post I found on [StackExchange] (http://datascience.stackexchange.com/questions/9364/hypertuning-xgboost-parameters).  I also experimented with other values of eta and nrounds, then averaged these model predictions to further reduce bias/variability.  I also applied an extremely randomized trees model to the training dataset with no features removed.  My plan was to run a quick random forest model and then use the variable importance function to eliminate some of the lesser/insignificant variables to avoid possible overfitting.  Use Amazon Web Services to compute the extremely randomized trees model (insert link to paper here).
