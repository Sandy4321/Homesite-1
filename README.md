# Homesite Kaggle Competition
This is a brief summary of my approach to solving the Homesite Quote Conversion Kaggle Competition.  The competition details can be found [here] (https://www.kaggle.com/c/homesite-quote-conversion).  My final rank was 298 of 1762.

##Feature Engineering
Feature engineering on this data was a bit challenging given that it was a blind dataset and little or no insight could be intuitively gleaned from the variables.  First, I reformatted the date column into three different categorical variables: month, year and day of the week.  Second, I came across an idea highlighted in the Kaggle Forum (credit [yilisg] (https://www.kaggle.com/c/homesite-quote-conversion/forums/t/18225/two-insights-for-0-96852)) to calculate the number of missing and/or zero-valued elements in each row.  Adding this feature made intuitive sense given that customers who are less inclined to correctly or completely fill out the insurance survey are also probably less inclined to be serious about purchasing an insurance policy.  I added this feature for both each entire row as well as binned subfields within each row (e.g., Property, Personal, etc.).  In total, the train and test datasets had 315 and 314 features, respectively.

##Variable Importance
An Extreme Gradient Boosting (XGB) model was first run on the feature engineered dataset in order to reduce the number of features based on their importance.  To achieve this I ran a basic XGB model with eta = 0.02 and nrounds = 2000.  In retrospect I believe the number of rounds was probably too high, but I nevertheless hoped (and assumed) the relative ranking of variable importance would not change too much if I had used fewer trees.  When assessing the variable importance I noticed some features (more than 20!) were not used during the boosting process, which was probably indicative of a high degree of collinearity among the variables.  I drew a somewhat arbitrary line on the gain and said any feature that did not contribute to a gain of at least 5e-5 was deemed not important enough to be included in the final dataset.  This process reduced the number of features by about 21%.  

##Model fitting
Once the feature engineering and variable selection were complete, I started with a 5-fold cross validation on the training dataset using an XGB model.  I held eta and nrounds fixed and ran a custom grid search on subsample, colsample_bytree and max.depth using the caret package in R.  The grid search and XGB parameter hypertuning were inspired by a post I found on [StackExchange] (http://datascience.stackexchange.com/questions/9364/hypertuning-xgboost-parameters).  I also experimented with other values of eta and nrounds, then averaged these model predictions to further reduce the variance.  I also applied an Extremely Randomized Trees model to the training dataset with no features removed.  A nice introduction to Extremely Randomized Trees as well as the Extra-Trees algorithm can be found [here] (http://www.montefiore.ulg.ac.be/~ernst/uploads/news/id63/extremely-randomized-trees.pdf). On a side note, I found the Extra-Trees model in R to be extremely memory intensive, so after crashing several times on my local machine (Java heap space errors) I switched to Amazon Web Services (AWS) to complete the computation.  My plan was to run a quick Random Forest model and then use the variable importance function to eliminate some of the lesser significant variables to avoid overfitting on both my Random Forest and Extremely Randomized Trees models. In the end, the Random Forest model took too long to run and did not complete before the competition finished. Therefore, I was not able to incorporate the Random Forest model or refine the input variables based on their importance. The final ensemble consisted of taking the arithmetic mean of several XGB models and a single Extremely Randomized Trees model. I believe the feature engineering and variable selection led to the greatest gains on the leaderboard. If I had more time (and computing power and expertise!) I would have also liked to use the h2o package in R to develop a deep learning model.    
